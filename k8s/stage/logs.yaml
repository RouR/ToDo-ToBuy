#https://github.com/flant/loghouse



---
kind: ConfigMap
apiVersion: v1
data:
  containers.input.conf: |-
    # Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    <source>
      type tail
      path /var/log/containers/*.log
      pos_file /var/log/containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag kubernetes.*
      format json
      keep_time_key true
      read_from_head true
    </source>
  system.input.conf: |-
    # Logs from systemd-journal for interesting services.
    <source>
      type systemd
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
      pos_file /var/log/gcp-journald-docker.pos
      read_from_head true
      tag docker
    </source>
    <source>
      type systemd
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      pos_file /var/log/gcp-journald-kubelet.pos
      read_from_head true
      tag kubelet
    </source>
  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      type forward
    </source>
  monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
    </source>
    <source>
      @type monitor_agent
    </source>
    # input plugin that collects metrics from MonitorAgent
    <source>
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
    # input plugin that collects metrics for in_tail plugin
    <source>
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
  output.conf: |-
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      type kubernetes_metadata
      merge_json_log false
    </filter>
    <filter kubernetes.**>
      @type record_modifier
      <record>
        _json_log_             ${ log = record["log"].strip; if log[0].eql?('{') && log[-1].eql?('}'); begin; JSON.parse(log); rescue JSON::ParserError; end; end }
        timestamp              ${time}
        nsec                   ${record["time"].split('.').last.to_i}
        # static fields
        source                 "kubernetes"
        namespace              ${record["kubernetes"]["namespace_name"]}
        host                   ${record["kubernetes"]["host"]}
        pod_name               ${record["kubernetes"]["pod_name"]}
        container_name         ${record["kubernetes"]["container_name"]}
        stream                 ${record["stream"]}
        # dynamic fields
        labels.names           ${record["kubernetes"]["labels"].keys}
        labels.values          ${record["kubernetes"]["labels"].values}
        string_fields.names    ${record["_json_log_"] ? record["_json_log_"].select{|_, v| !v.nil? && !v.is_a?(Numeric) && !v.is_a?(TrueClass) && !v.is_a?(FalseClass)}.keys : ["log"]}
        string_fields.values   ${record["_json_log_"] ? record["_json_log_"].select{|_, v| !v.nil? && !v.is_a?(Numeric) && !v.is_a?(TrueClass) && !v.is_a?(FalseClass)}.values.map(&:to_s) : [record["log"]]}
        number_fields.names    ${record["_json_log_"] ? record["_json_log_"].select{|_, v| v.is_a?(Numeric)}.keys : []}
        number_fields.values   ${record["_json_log_"] ? record["_json_log_"].select{|_, v| v.is_a?(Numeric)}.values : []}
        boolean_fields.names   ${record["_json_log_"] ? record["_json_log_"].select{|_, v| v.is_a?(TrueClass) || v.is_a?(FalseClass)}.keys : []}
        boolean_fields.values  ${record["_json_log_"] ? record["_json_log_"].select{|_, v| v.is_a?(TrueClass) || v.is_a?(FalseClass)}.values.map{|v| v ? 1 : 0} : []}
        null_fields.names      ${record["_json_log_"] ? record["_json_log_"].select{|_, v| v.nil?}.keys : []}
      </record>
      remove_keys kubernetes, docker, master_url, time, log, _json_log_
     </filter>
    <filter docker.**>
      @type record_modifier
      <record>
        timestamp              ${time}
        source                 "docker"
        host                   ${record["_HOSTNAME"]}
        stream                 ${record["_TRANSPORT"]}
        string_fields.names    ${["log"]}
        string_fields.values   ${[record["MESSAGE"]]}
      </record>
      remove_keys _TRANSPORT, PRIORITY, SYSLOG_FACILITY, _UID, _GID, _CAP_EFFECTIVE, _SYSTEMD_SLICE, _BOOT_ID, _MACHINE_ID, _HOSTNAME, SYSLOG_IDENTIFIER, _PID, _COMM, _EXE, _CMDLINE, _SYSTEMD_CGROUP, _SYSTEMD_UNIT, MESSAGE
    </filter>
    <filter kubelet.**>
      @type record_modifier
      <record>
        timestamp              ${time}
        source                 "kubelet"
        host                   ${record["_HOSTNAME"]}
        stream                 ${record["_TRANSPORT"]}
        string_fields.names    ${["log"]}
        string_fields.values   ${[record["MESSAGE"]]}
      </record>
      remove_keys _TRANSPORT, PRIORITY, SYSLOG_FACILITY, _UID, _GID, _CAP_EFFECTIVE, _SYSTEMD_SLICE, _BOOT_ID, _MACHINE_ID, _HOSTNAME, SYSLOG_IDENTIFIER, _PID, _COMM, _EXE, _CMDLINE, _SYSTEMD_CGROUP, _SYSTEMD_UNIT, MESSAGE
    </filter> 
    <match **>
      @type exec
      command bash /usr/local/bin/insert_ch.sh
      format json
      buffer_type memory
      buffer_chunk_limit 64m
      buffer_queue_limit 32
      flush_at_shutdown true
      flush_interval 1s
      num_threads 4
    </match>

metadata:
  name: fluentd-config
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
    
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    k8s-app: fluentd
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: fluentd
  labels:
    k8s-app: fluentd
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "namespaces"
  - "pods"
  verbs:
  - "get"
  - "watch"
  - "list"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: fluentd
  labels:
    k8s-app: fluentd
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile 
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: kube-system
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: ""    
  
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
  labels:
    k8s-app: fluentd
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        k8s-app: fluentd
        kubernetes.io/cluster-service: "true"
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: flant/loghouse-fluentd:0.0.2
        imagePullPolicy: IfNotPresent
        env:
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        - name: CLICKHOUSE_SERVER
          value: "clickhouse"
        - name: CLICKHOUSE_PORT
          value: "9000"
        - name: CLICKHOUSE_USER
          value: "default"
        - name: CLICKHOUSE_PASS
          value: "password"
        - name: CLICKHOUSE_DB
          value: "logs"
        - name: K8S_LOGS_TABLE
          value: "logs"
#        resources:
#          limits:
#            memory: 100m
#            cpu: 512Mi
#          requests:
#            memory: 50m
#            cpu: 256Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: libsystemddir
          mountPath: /host/lib
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: "node-role/logging"
        operator: "Exists"
      - key: "node-role/system"
        operator: "Exists"
      - key: "node-role/monitoring"
        operator: "Exists"
      - key: "node-role/ingress"
        operator: "Exists"
      - key: "node-role/frontend"
        operator: "Exists"
      - key: "dedicated"
        operator: "Exists"

      - key: "node-role.kubernetes.io/master"
        operator: "Exists"

      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: libsystemddir
        hostPath:
          path: /usr/lib64
      - name: config-volume
        configMap:
          name: fluentd-config
          
          
---






---
kind: ConfigMap
apiVersion: v1
metadata:
  name: loghouse-user-config
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:
  user.conf: |-
    admin:
      - ".*"        

      
      
---
apiVersion: batch/v2alpha1
kind: CronJob
metadata:
  name: logs-tables
  namespace: kube-system
spec:
  schedule: "59 23 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          initContainers:
          - name: wait-clickhouse
            image: alpine:3.6
            command: ['/bin/sh', '-c', 'while ! getent ahostsv4 clickhouse; do sleep 1; done']
          containers:
            - name: cron
              image: flant/loghouse-dashboard:0.0.2
              imagePullPolicy: IfNotPresent
              command: ['/bin/bash', '-l', '-c', 'rake create_logs_tables']
              env:
              - name: KUBERNETES_DEPLOYED
                value: "now"
              - name: CLICKHOUSE_URL
                value: "clickhouse:8123"
              - name: CLICKHOUSE_USERNAME
                value: "default"
              - name: CLICKHOUSE_PASSWORD
                value: "password"
              - name: CLICKHOUSE_DATABASE
                value: "logs"
# Custom partitioning, using LOGS_TABLES_PARTITION_PERIOD variable, in hours.
# 24 must be divisible by this value for now, so tables always can start with 00 hours (it's skipped for 24). Default is 24, as it was.                
              - name: LOGS_TABLES_PARTITION_PERIOD
                value: "1"
              - name: CLICKHOUSE_LOGS_TABLE
                value: "logs"


---
apiVersion: batch/v1
kind: Job
metadata:
  name: logs-init-db
  namespace: kube-system
spec:
  activeDeadlineSeconds: 300
  template:
    metadata:
      name: logs-init-db
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-clickhouse
        image: alpine:3.6
        command: [ '/bin/sh', '-c', 'while ! getent ahostsv4 clickhouse; do sleep 1; done' ]
      containers:
      - name: init
        image: flant/loghouse-fluentd:0.0.2
        imagePullPolicy: IfNotPresent
        command: ['/bin/bash', '-l', '-c', 'clickhouse-client --host=${CLICKHOUSE_URL} --port=9000 --user=${CLICKHOUSE_USERNAME} --password=${CLICKHOUSE_PASSWORD} --query="CREATE DATABASE ${CLICKHOUSE_DATABASE};"']
        env:
        - name: KUBERNETES_DEPLOYED
          value: "now"
        - name: CLICKHOUSE_URL
          value: "clickhouse"
        - name: CLICKHOUSE_USERNAME
          value: "default"
        - name: CLICKHOUSE_PASSWORD
          value: "password"
        - name: CLICKHOUSE_DATABASE
          value: "logs"
        - name: CLICKHOUSE_LOGS_TABLE
          value: "logs"
        - name: LOGS_TABLES_PARTITION_PERIOD
          value: "1"
        - name: RACK_ENV
          value: "production"     

---
apiVersion: batch/v1
kind: Job
metadata:
  name: logs-init-tables
  namespace: kube-system
spec:
  activeDeadlineSeconds: 160
  template:
    metadata:
      name: logs-init-tables
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-clickhouse
        image: alpine:3.6
        command: [ '/bin/sh', '-c', 'while ! getent ahostsv4 clickhouse; do sleep 1; done' ]
      containers:
      - name: init-tables
        image: flant/loghouse-dashboard:0.0.2
        imagePullPolicy: IfNotPresent
        command: ['/bin/bash', '-l', '-c', 'rake create_logs_tables']
        env:
        - name: KUBERNETES_DEPLOYED
          value: "now"
        - name: CLICKHOUSE_URL
          value: "clickhouse:8123"
        - name: CLICKHOUSE_USERNAME
          value: "default"
        - name: CLICKHOUSE_PASSWORD
          value: "password"
        - name: CLICKHOUSE_DATABASE
          value: "logs"
        - name: CLICKHOUSE_LOGS_TABLE
          value: "logs"
        - name: LOGS_TABLES_PARTITION_PERIOD
          value: "1"
        - name: RACK_ENV
          value: "production" 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: loghouse
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: loghouse
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
  - list
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: loghouse
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: loghouse
subjects:
- kind: ServiceAccount
  name: loghouse
  namespace: kube-system
  
  
  
  
  
  
  
  
  
  
  
  
---   
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: tabix
  namespace: kube-system
spec:
  revisionHistoryLimit: 1
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        component: tabix
    spec:
      containers:
      - name: tabix
        image: flant/loghouse-tabix:0.0.2
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
        livenessProbe:
          timeoutSeconds: 1
          initialDelaySeconds: 60
          tcpSocket:
            port: 80
        readinessProbe:
          timeoutSeconds: 1
          initialDelaySeconds: 5
          tcpSocket:
            port: 80
        resources:
          requests:
            cpu: 0.2
            memory: 256Mi
          limits:
            cpu: 0.5
            memory: 512Mi
---
# apiVersion: v1
# kind: Service
# metadata:
  # name: tabix
  # namespace: kube-system
# spec:
  # selector:
    # component: tabix
  # ports:
  # - name: http
    # port: 80
    # targetPort: 80
    # protocol: TCP       


---
# apiVersion: v1
# kind: Service
# metadata:
  # name: clickhouse
  # namespace: kube-system
# spec:
  # selector:
    # component: clickhouse
  # ports:
  # - name: http
    # port: 8123
    # targetPort: 8123
    # protocol: TCP
  # - name: native
    # port: 9000
    # targetPort: 9000
    # protocol: TCP  
  

---


kind: Endpoints
apiVersion: v1
metadata:  
  name: clickhouse
  namespace: kube-system
subsets:
  - addresses:
      # InternalIP of node with clickhouse, outside the cluster
      - ip: 207.244.95.63
    ports:
      - port: 8123
        name: nn1
      - port: 9000
        name: nn2
    